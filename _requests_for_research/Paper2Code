---
title: 'Paper2Code'
summary: 'Given CS/ML paper describing a model, generate the model's source code. Somewhat insane, but impactful/solvable.'
difficulty: 3 # out of 3
---


<p> <a href="http://arxiv.org/pdf/1603.06744.pdf">Code Generators</a> conditioned on various input types have yielded promising results. Recent work has shown that neural networks (that combine multiple predictors) are capable of learning to output executable code when given a textual (and/or visual?) description of a model's functional specification. </p>


<p> Implement a model that takes in an image (or maybe text/latex) of a (machine_learning?) paper from arxiv and outputs the characters of an executable code that satisfies the functional specifications described in the paper. </p>

<hr />

<h3>Getting Started</h3>

<p> To get started, proceed according to the following stages: </p>

  <ul>
    <li> Download a large number of papers-code pairs from <a href="http://gitxiv.com">GitXiv</a> for the purpose of training/testing. Try to use pairs that have the code written in the same language for all the pairs (the language should be one that is adopted by a substantial portion of the community). It would be convenient if someone (such as maybe <a href="http://samim.io/">Samim</a>) first created an easily downloadable dataset of arxiv_paper-source_code pairs.  The bandwidth of arXiv is <a href='https://arxiv.org/help/bulk_data'>limited</a>, so be mindful of arXiv's constraints and do not write crawlers to download an excessive number of arxiv papers at once. </li>
    <li> Will need a reasoning component such as <a href="https://arxiv.org/pdf/1603.01417.pdf">(Dynamic?) Memory Networks </a> (or <a href="http://arxiv.org/pdf/1603.08884.pdf">Parallel-Hierarchical Models </a> in the case of scaling to whole documents) to reason about meaning of and iteratively attend to memories of text/images within the paper. A <a href='https://github.com/ethancaballero/Improved-Dynamic-Memory-Networks-DMN-plus'>Theano implementation</a> of the <a href="https://arxiv.org/pdf/1603.01417.pdf">Dynamic Memory Networks for textual and visual qa</a> paper can help you get started. </li>
    <li> Will need an interface-interaction component such as <a href="http://arxiv.org/pdf/1505.00521.pdf">Reinforcement Learning Neural Turing Machines</a> to interact with external interface such as google's search engine in order to look up relevant concepts that are cited but not explained in the paper. </li>
    <li> Will need a generative component such as <a href="http://arxiv.org/pdf/1603.06744.pdf">Latent Predictor Networks for Code Generation</a> to generate code. Employing symbol compression techniques and multiple predictors (such as char-level generators and keyword copiers) as described in Latent Predictor Network paper will probably make this step more feasible/scalable. </li>
 </ul>

<hr />

<h3>Notes</h3>

<p> Success would drastically increase the speed/openness of research in that source code for papers would be available immediately after papers are released.</p>

<p> This project is non-trivial (three-star difficulty) in that it requires some current methods to be improved/combined.</p>
