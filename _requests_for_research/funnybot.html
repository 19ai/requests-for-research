---
title: Comedian language model
summary: ''
difficulty: 2 # out of 3
---

<p>Train a language model capable of generating jokes from one of 
predefined categories.
This request can be solved in the following steps:
</p>

<p> 
Firstly, obtain a large corpus of jokes in raw text format, that will 
later be used to train language model. For initial tests you can use 
the following existing datasets:
</p>
<p> 
Pun of the Day dataset: ~2500 puns; 16000 One Liners dataset: ~16000 
one line jokes; See 
[Yang, Diyi, et al. "Humor recognition and humor anchor extraction."](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.1901&rep=rep1&type=pdf)
</p>
<p> 
[Jester datasets](http://eigentaste.berkeley.edu/dataset/) : contains around 
150 jokes and ratings from large number of users for these jokes.
</p>
<p>
Train a large [language model](https://arxiv.org/abs/1602.02410) 
on jokes datasets, similarily as [in this post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).
See if the model trained on above datasets produces reasonable results.
</p>
<p>
Most likely to obtainin a reasonable language model more
data is required then what is available in above datasets. 
Obtain such additional data by web scraping sites like 
https://www.reddit.com/r/jokes, 
http://funtweets.com/, 
http://funnytweeter.com/ 
and similar sites. 
Please make sure to obey the website policies with respect to the web scraping!
For reddit comments in general, you can use [this torrent](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/).
</p>
<p>
To further increase the amount of text that language model was trained on,
pretrain a language model on large corpus of regular text, for example
on [One Billion Words dataset](http://arxiv.org/abs/1312.3005), which you can 
download [here](http://www.statmt.org/lm-benchmark/). This way the language
model will already be initialized with knowledge of general structures in 
English language; Fine tune such pretrained model on the jokes corpus. 
One of the outcomes of this research request is to determine whether pretraining
helps with joke generation.
</p>
<p>
People are all different, and so are their tastes in jokes, thus some
might prefer a certain category of jokes over others. Modify the language
model obtained in previous steps such that it can be configured to generate
jokes from a certain category only. To do so, train language model using jokes 
from https://www.reddit.com/r/jokes on both joke text and one hot encoded 
label of joke, such that language model can be configured to generate jokes
only of certain type by fixing the corresponding input value encoding label.
For the other datasets, the jokes can be labeled using a text classifier
trained to detect the reddit label from the joke text.  
</p>
<p>
Expected outcome of this research request is to determine whether a reasonable 
language model described in the previous paragraph can be built with current language 
modelling approaches.
</p>

<p>Related literature:
[etrovic, Sasa, and David Matthews. "Unsupervised joke generation from big data." ACL (2). 2013.](http://homepages.inf.ed.ac.uk/s0894589/petrovic13unsupervised.pdf)
</p>

<p>Potential follow up after this research request is solved is to create a 
setup in which created neural network is able to
post the jokes somewhere online where it can get reliable score feedback
for the jokes it generates and improve itself accordingly using RL.
</p>
